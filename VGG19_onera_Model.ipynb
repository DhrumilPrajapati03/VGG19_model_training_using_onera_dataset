{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/DhrumilPrajapati03/VGG19_model_training_using_onera_dataset/blob/main/VGG19_onera_Model.ipynb",
      "authorship_tag": "ABX9TyMLOO2pxCK51c9gSYTm0kIL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhrumilPrajapati03/VGG19_model_training_using_onera_dataset/blob/main/VGG19_onera_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "o82EiAgAOLvw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define directories\n",
        "ROOT_DIR = r\"/content/drive/MyDrive/Onera_dataset\"\n",
        "IMAGE_DIR = os.path.join(ROOT_DIR, \"Onera Satellite Change Detection dataset - Images\",\"Onera Satellite Change Detection dataset - Images\")\n",
        "TRAIN_LABELS_DIR = os.path.join(ROOT_DIR, \"Onera Satellite Change Detection dataset - Train Labels\",\"Onera Satellite Change Detection dataset - Train Labels\")\n",
        "TEST_LABELS_DIR = os.path.join(ROOT_DIR, \"Onera Satellite Change Detection dataset - Test Labels\",\"Onera Satellite Change Detection dataset - Test Labels\")\n",
        "\n",
        "print(IMAGE_DIR)\n",
        "print(TRAIN_LABELS_DIR)\n",
        "print(TEST_LABELS_DIR)"
      ],
      "metadata": {
        "id": "OTybXO9dWjUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "494d6d52-6d73-4174-db1a-9afc359555e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Onera_dataset/Onera Satellite Change Detection dataset - Images/Onera Satellite Change Detection dataset - Images\n",
            "/content/drive/MyDrive/Onera_dataset/Onera Satellite Change Detection dataset - Train Labels/Onera Satellite Change Detection dataset - Train Labels\n",
            "/content/drive/MyDrive/Onera_dataset/Onera Satellite Change Detection dataset - Test Labels/Onera Satellite Change Detection dataset - Test Labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset paths\n",
        "def load_dataset_paths(city_folder):\n",
        "  img1_dir = os.path.join(city_folder, \"imgs_1_rect\")\n",
        "  img2_dir = os.path.join(city_folder, \"imgs_2_rect\")\n",
        "\n",
        "  img1_files = sorted(os.listdir(img1_dir))\n",
        "  img2_files = sorted(os.listdir(img2_dir))\n",
        "\n",
        "  pairs = []\n",
        "  for img1_file, img2_file in zip(img1_files, img2_files):\n",
        "    img1_path = os.path.join(img1_dir, img1_file)\n",
        "    img2_path = os.path.join(img2_dir, img2_file)\n",
        "    pairs.append((img1_path, img2_path))\n",
        "\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "XLu5usrJX8Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess images and labels\n",
        "def preprocess_images(img1_path, img2_path, label_path, target_size=(224, 224)):\n",
        "    img1 = load_img(img1_path, target_size=target_size)\n",
        "    img2 = load_img(img2_path, target_size=target_size)\n",
        "    label = load_img(label_path, target_size=target_size, color_mode=\"grayscale\")\n",
        "\n",
        "    img1 = img_to_array(img1)\n",
        "    img2 = img_to_array(img2)\n",
        "    label = img_to_array(label)\n",
        "\n",
        "    img1 = preprocess_input(img1)\n",
        "    img2 = preprocess_input(img2)\n",
        "    label = label / 255.0  # Normalize label to [0, 1]\n",
        "\n",
        "    return img1, img2, label"
      ],
      "metadata": {
        "id": "mjOCkh3wZEaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GayK8T82OGOI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "8f4c1b91-7c46-4159-ae56-6743e620ec19"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6c75ca3b03b2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Split the dataset into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mX1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Build and compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "# Build the VGG19-based change detection model\n",
        "def build_change_detection_model(input_shape=(224, 224, 3)):\n",
        "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Freeze the base model\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Inputs for two images\n",
        "    input1 = Input(shape=input_shape)\n",
        "    input2 = Input(shape=input_shape)\n",
        "\n",
        "    # Extract features from both images\n",
        "    features1 = base_model(input1)\n",
        "    features2 = base_model(input2)\n",
        "\n",
        "    # Compute absolute difference between features\n",
        "    diff = tf.abs(features1 - features2)\n",
        "\n",
        "    # Flatten and add dense layers\n",
        "    x = Flatten()(diff)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)  # Binary classification (change/no change)\n",
        "\n",
        "    model = Model(inputs=[input1, input2], outputs=output)\n",
        "    return model\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "image_pairs, labels = load_dataset_paths(IMAGE_DIR, TRAIN_LABELS_DIR)\n",
        "X1, X2, y = [], [], []\n",
        "for img1_path, img2_path in image_pairs:\n",
        "    img1, img2, label = preprocess_images(img1_path, img2_path, labels[image_pairs.index((img1_path, img2_path))])\n",
        "    X1.append(img1)\n",
        "    X2.append(img2)\n",
        "    y.append(label)\n",
        "\n",
        "X1 = np.array(X1)\n",
        "X2 = np.array(X2)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(X1, X2, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and compile the model\n",
        "model = build_change_detection_model()\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X1_train, X2_train], y_train,\n",
        "    validation_data=([X1_val, X2_val], y_val),\n",
        "    batch_size=16,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_image_pairs, test_labels = load_dataset_paths(IMAGE_DIR, TEST_LABELS_DIR)\n",
        "X1_test, X2_test, y_test = [], [], []\n",
        "for img1_path, img2_path in test_image_pairs:\n",
        "    img1, img2, label = preprocess_images(img1_path, img2_path, test_labels[test_image_pairs.index((img1_path, img2_path))])\n",
        "    X1_test.append(img1)\n",
        "    X2_test.append(img2)\n",
        "    y_test.append(label)\n",
        "\n",
        "X1_test = np.array(X1_test)\n",
        "X2_test = np.array(X2_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate([X1_test, X2_test], y_test, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bCVEPOlebimT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}